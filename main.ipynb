{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8dcbed-cbf7-4490-afe8-94645586cc66",
   "metadata": {},
   "source": [
    "# Load required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952959b-949f-4a47-9aaf-0291aa537289",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "To install the packages required for this notebook on the HPC, please follow the 'Jupyter Kernel Creation' slides posted on OPAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92f404-8c4f-4c8f-94fb-fcf3a25df16e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from datasets import Dataset as HFDataset, load_dataset\n",
    "\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244165b2-462b-490e-a6de-0355171cd2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f37a06-0f9d-4a48-ba11-db02b031a9e5",
   "metadata": {},
   "source": [
    "# Load the model (Llama-8B or Mistral-7B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a6300-b87d-4b5c-a9c1-1deb92764c99",
   "metadata": {},
   "source": [
    "Note that you need to be on the partition with GPU (e.g. capella, alpha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ff630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device for GPU acceleration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e0a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa7113",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f400f49f",
   "metadata": {},
   "source": [
    "# Build Wikipedia RAG Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70544710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Wikipedia RAG index\n",
    "\n",
    "COUNTRY_KEYWORDS = {\n",
    "    \"IR\": [\n",
    "        \"iran\", \"iranian\", \"persia\", \"persian\", \"tehran\", \"isfahan\", \"shiraz\",\n",
    "        \"nowruz\", \"farsi\", \"shia\", \"zoroastrian\", \"rials\", \"ayatollah\"\n",
    "    ],\n",
    "    \"CN\": [\n",
    "        \"china\", \"chinese\", \"beijing\", \"shanghai\", \"mandarin\", \"dynasty\",\n",
    "        \"confucius\", \"yuan\", \"lunar new year\", \"spring festival\", \"cantonese\"\n",
    "    ],\n",
    "    \"GB\": [\n",
    "        \"united kingdom\", \"british\", \"britain\", \"england\", \"english\", \"london\",\n",
    "        \"scotland\", \"scottish\", \"wales\", \"welsh\", \"pound sterling\", \"parliament\"\n",
    "    ],\n",
    "    \"US\": [\n",
    "        \"united states\", \"american\", \"america\", \"washington\", \"new york\",\n",
    "        \"thanksgiving\", \"fourth of july\", \"dollar\", \"congress\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "GENERAL_CULTURE_KEYWORDS = [\n",
    "    \"culture\", \"tradition\", \"festival\", \"food\", \"music\", \"art\", \"history\",\n",
    "    \"religion\", \"language\", \"custom\", \"heritage\", \"cuisine\", \"education\",\n",
    "    \"sport\", \"holiday\", \"wedding\", \"family\", \"school\", \"university\"\n",
    "]\n",
    "\n",
    "ALL_KEYWORDS = list(set(\n",
    "    [kw for keywords in COUNTRY_KEYWORDS.values() for kw in keywords] +\n",
    "    GENERAL_CULTURE_KEYWORDS\n",
    "))\n",
    "\n",
    "\n",
    "class WikipediaRAG:\n",
    "    \"\"\"RAG system using Wikipedia as knowledge source with country-aware retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        reranker_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "        enable_rerank: bool = True\n",
    "    ):\n",
    "        self.encoder = SentenceTransformer(embedding_model)\n",
    "        self.reranker = CrossEncoder(reranker_model) if enable_rerank else None\n",
    "        self.index = None\n",
    "        self.passages = []\n",
    "        self.passage_countries = []\n",
    "        \n",
    "    def build_index(self, passages: List[str], countries: List[str] = None, batch_size: int = 64):\n",
    "        \"\"\"Build FAISS index from passages with optional country metadata.\"\"\"\n",
    "        self.passages = passages\n",
    "        self.passage_countries = countries if countries else [\"general\"] * len(passages)\n",
    "        \n",
    "        embeddings = self.encoder.encode(passages, show_progress_bar=True, batch_size=batch_size)\n",
    "        embeddings = np.array(embeddings).astype('float32')\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        \n",
    "        self.index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "    def save_index(self, path: str):\n",
    "        \"\"\"Save index and passages to disk.\"\"\"\n",
    "        import os\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        faiss.write_index(self.index, f\"{path}/wiki_index.faiss\")\n",
    "        with open(f\"{path}/wiki_passages.json\", \"w\") as f:\n",
    "            json.dump({\"passages\": self.passages, \"countries\": self.passage_countries}, f)\n",
    "        \n",
    "    def load_index(self, path: str):\n",
    "        \"\"\"Load index and passages from disk.\"\"\"\n",
    "        self.index = faiss.read_index(f\"{path}/wiki_index.faiss\")\n",
    "        with open(f\"{path}/wiki_passages.json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            self.passages = data[\"passages\"]\n",
    "            self.passage_countries = data.get(\"countries\", [\"general\"] * len(self.passages))\n",
    "        \n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        country: str = None,\n",
    "        rerank_k: int = 20\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Retrieve top-k relevant passages, optionally filtering by country, then rerank.\"\"\"\n",
    "        query_embedding = self.encoder.encode([query]).astype('float32')\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        search_k = max(top_k * 5, rerank_k) if country else max(top_k, rerank_k)\n",
    "        scores, indices = self.index.search(query_embedding, search_k)\n",
    "        \n",
    "        country_results = []\n",
    "        general_results = []\n",
    "        other_results = []\n",
    "        \n",
    "        for idx, score in zip(indices[0], scores[0]):\n",
    "            if 0 <= idx < len(self.passages):\n",
    "                passage_country = self.passage_countries[idx]\n",
    "                item = (self.passages[idx], float(score))\n",
    "                \n",
    "                if country and passage_country == country:\n",
    "                    country_results.append(item)\n",
    "                elif passage_country == \"general\":\n",
    "                    general_results.append(item)\n",
    "                else:\n",
    "                    other_results.append(item)\n",
    "        \n",
    "        final_results = country_results + general_results + other_results\n",
    "        candidates = final_results[:max(rerank_k, top_k)]\n",
    "        \n",
    "        if self.reranker and candidates:\n",
    "            pairs = [[query, passage] for passage, _ in candidates]\n",
    "            rerank_scores = self.reranker.predict(pairs)\n",
    "            ranked = sorted(zip(candidates, rerank_scores), key=lambda x: x[1], reverse=True)\n",
    "            return [(passage, float(score)) for (passage, _), score in ranked[:top_k]]\n",
    "        \n",
    "        return final_results[:top_k]\n",
    "\n",
    "\n",
    "def build_wiki_rag_index():\n",
    "    \"\"\"Load Wikipedia and build the RAG index.\"\"\"\n",
    "    \n",
    "    wiki_dataset = load_dataset(\n",
    "        \"wikimedia/wikipedia\", \n",
    "        \"20231101.simple\",\n",
    "        split=\"train\"\n",
    "    )\n",
    "    \n",
    "    def get_article_country(article) -> str:\n",
    "        title_lower = article[\"title\"].lower()\n",
    "        text_lower = article[\"text\"][:2000].lower()\n",
    "        combined = title_lower + \" \" + text_lower\n",
    "        \n",
    "        scores = {}\n",
    "        for country, keywords in COUNTRY_KEYWORDS.items():\n",
    "            score = sum(1 for kw in keywords if kw in combined)\n",
    "            if score > 0:\n",
    "                scores[country] = score\n",
    "        \n",
    "        return max(scores, key=scores.get) if scores else \"general\"\n",
    "    \n",
    "    def is_relevant_article(article) -> bool:\n",
    "        title_lower = article[\"title\"].lower()\n",
    "        text_lower = article[\"text\"][:1000].lower()\n",
    "        return any(kw in title_lower or kw in text_lower for kw in ALL_KEYWORDS)\n",
    "    \n",
    "    def chunk_article(article, chunk_size=300, overlap=50) -> List[dict]:\n",
    "        title = article[\"title\"]\n",
    "        text = article[\"text\"]\n",
    "        country = article.get(\"detected_country\", \"general\")\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunks.append({\n",
    "                \"text\": f\"[{title}] {text[start:end]}\",\n",
    "                \"country\": country,\n",
    "                \"title\": title\n",
    "            })\n",
    "            start = end - overlap\n",
    "            if len(text) - start < 100:\n",
    "                break\n",
    "        return chunks\n",
    "    \n",
    "    cultural_articles = []\n",
    "    for a in wiki_dataset:\n",
    "        if is_relevant_article(a):\n",
    "            cultural_articles.append({**a, \"detected_country\": get_article_country(a)})\n",
    "    \n",
    "    wiki_passages = []\n",
    "    wiki_passage_countries = []\n",
    "    \n",
    "    for article in cultural_articles:\n",
    "        chunks = chunk_article(article)\n",
    "        for chunk in chunks:\n",
    "            wiki_passages.append(chunk[\"text\"])\n",
    "            wiki_passage_countries.append(chunk[\"country\"])\n",
    "    \n",
    "    rag = WikipediaRAG()\n",
    "    rag.build_index(wiki_passages, wiki_passage_countries)\n",
    "    \n",
    "    rag.save_index(\"./wiki_rag_index\")\n",
    "    \n",
    "    return rag\n",
    "\n",
    "\n",
    "import os\n",
    "if os.path.exists(\"./wiki_rag_index/wiki_index.faiss\"):\n",
    "    wiki_rag = WikipediaRAG()\n",
    "    wiki_rag.load_index(\"./wiki_rag_index\")\n",
    "    print(f\"Loaded RAG index with {wiki_rag.index.ntotal} passages\")\n",
    "else:\n",
    "    wiki_rag = build_wiki_rag_index()\n",
    "    print(f\"Built RAG index with {wiki_rag.index.ntotal} passages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common utilities for SAQ and MCQ\n",
    "\n",
    "COUNTRY_CODE_MAP = {\n",
    "    \"Iran\": \"IR\",\n",
    "    \"China\": \"CN\",\n",
    "    \"UK\": \"GB\",\n",
    "    \"United Kingdom\": \"GB\",\n",
    "    \"US\": \"US\",\n",
    "    \"United States\": \"US\",\n",
    "}\n",
    "\n",
    "def clear_gpu():\n",
    "    \"\"\"Free GPU memory between training/inference stages.\"\"\"\n",
    "    import gc\n",
    "    for name in [\"model\", \"tokenizer\", \"saq_trainer\", \"mcq_trainer\"]:\n",
    "        try:\n",
    "            del globals()[name]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def load_base_model(adapter_path: str):\n",
    "    \"\"\"Load base model and attach a LoRA adapter.\"\"\"\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    from peft import PeftModel\n",
    "    return PeftModel.from_pretrained(base_model, adapter_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c60c6",
   "metadata": {},
   "source": [
    "# SAQ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11fc020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAQ training data\n",
    "\n",
    "saq_df = pd.read_csv(\"train_dataset_saq.csv\")\n",
    "\n",
    "def extract_answers(annotations_str):\n",
    "    \"\"\"Extract English answers from annotations JSON string.\"\"\"\n",
    "    try:\n",
    "        annotations = ast.literal_eval(annotations_str)\n",
    "    except Exception:\n",
    "        return []\n",
    "    answers = []\n",
    "    for ann in annotations:\n",
    "        if \"en_answers\" in ann and ann[\"en_answers\"]:\n",
    "            answers.extend(ann[\"en_answers\"])\n",
    "    return list({a.strip() for a in answers if a.strip()})\n",
    "\n",
    "saq_df[\"en_answers\"] = saq_df[\"annotations\"].apply(extract_answers)\n",
    "saq_df = saq_df[saq_df[\"en_answers\"].map(len) > 0].reset_index(drop=True)\n",
    "\n",
    "print(\"Country distribution:\")\n",
    "print(saq_df['country'].value_counts())\n",
    "print(f\"\\nTotal SAQ training samples: {len(saq_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAQ dataset class with few-shot examples\n",
    "\n",
    "COUNTRY_NAMES = {\"IR\": \"Iran\", \"GB\": \"UK\", \"CN\": \"China\", \"US\": \"US\"}\n",
    "\n",
    "SAQ_FEW_SHOT = \"\"\"Example:\n",
    "Question: What is the traditional New Year celebration in Iran?\n",
    "Answer: Nowruz\n",
    "\n",
    "Example:\n",
    "Question: What is the most popular sport in the UK?\n",
    "Answer: Football\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class SAQDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"SAQ Dataset with few-shot examples using apply_chat_template for proper Mistral formatting.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, tokenizer, max_length=384):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            question = row[\"en_question\"]\n",
    "            valid_answers = row[\"en_answers\"]\n",
    "            \n",
    "            for answer in valid_answers:\n",
    "                self.data.append({\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "\n",
    "        user_message = (\n",
    "            \"Read the following question and provide a single answer \"\n",
    "            \"without any explanations.\\n\\n\"\n",
    "            f\"{SAQ_FEW_SHOT}\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        prompt_str = self.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        full_text = prompt_str + answer + self.tokenizer.eos_token\n",
    "\n",
    "        tokenized_full = self.tokenizer(\n",
    "            full_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized_full[\"input_ids\"][0]\n",
    "        attention_mask = tokenized_full[\"attention_mask\"][0]\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        tokenized_prompt = self.tokenizer(\n",
    "            prompt_str,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        prompt_len = min(tokenized_prompt[\"input_ids\"].shape[1], self.max_length)\n",
    "\n",
    "        labels[:prompt_len] = -100\n",
    "        labels[attention_mask == 0] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "test_msg = (\n",
    "    \"Read the following question and provide a single answer \"\n",
    "    \"without any explanations.\\n\\n\"\n",
    "    f\"{SAQ_FEW_SHOT}\"\n",
    "    \"Question: What is the capital of China?\\n\"\n",
    "    \"Answer:\"\n",
    " )\n",
    "print(\"Sample SAQ prompt:\")\n",
    "print(tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": test_msg}], tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc438cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and preview SAQ dataset\n",
    "saq_dataset = SAQDataset(saq_df, tokenizer, max_length=256)\n",
    "\n",
    "sample = saq_dataset[0]\n",
    "print(f\"Created dataset with {len(saq_dataset)} samples\")\n",
    "print(f\"Sample input (first 50 tokens): {tokenizer.decode(sample['input_ids'][:50])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59285aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SAQ LoRA adapter\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "saq_lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, saq_lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318dee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SAQ training arguments\n",
    "saq_training_args = TrainingArguments(\n",
    "    output_dir=\"./saq-lora-clean\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "saq_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=saq_training_args,\n",
    "    train_dataset=saq_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c781d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SAQ model\n",
    "saq_trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./saq-lora-adapter-clean\")\n",
    "tokenizer.save_pretrained(\"./saq-lora-adapter-clean\")\n",
    "print(\"SAQ adapter saved to ./saq-lora-adapter-clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48859099",
   "metadata": {},
   "source": [
    "# SAQ Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b32b9-d334-49b0-a45b-3484e5fc01f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_base_model(\"./saq-lora-adapter-clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ecb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web search utilities\n",
    "\n",
    "from ddgs import DDGS\n",
    "import string\n",
    "\n",
    "COUNTRY_SEARCH_NAMES = {\n",
    "    \"IR\": \"Iran\",\n",
    "    \"CN\": \"China\", \n",
    "    \"GB\": \"United Kingdom\",\n",
    "    \"US\": \"United States\"\n",
    "}\n",
    "\n",
    "def generate_search_query(question, model, tokenizer, country: str = None):\n",
    "    \"\"\"Generate a keyword-based search query from the question, with optional country appended.\"\"\"\n",
    "    prompt = f\"[INST] Generate from the question keywords. Only include keywords that are important. \\n Example: What is the most popular food in France? Answer: most,popular,food,france \\n\\nQuestion: {question}\\n\\nSearch Query: [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=20, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "        query = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True).strip()\n",
    "        query = query.replace('\"', '').replace(\"'\", \"\").replace(\"Query:\", \"\").strip()\n",
    " \n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    raw_tags = query.split(',')\n",
    "    validated_tags = []\n",
    "    \n",
    "    for tag in raw_tags:\n",
    "        clean_tag = tag.strip()\n",
    "        if not clean_tag:\n",
    "            continue\n",
    "            \n",
    "        clean_tag = clean_tag.split()[0]\n",
    "        \n",
    "        clean_tag = clean_tag.strip(string.punctuation)\n",
    "        \n",
    "        if not clean_tag:\n",
    "            continue\n",
    "\n",
    "        if re.search(r'\\b' + re.escape(clean_tag.lower()) + r'\\b', question_lower):\n",
    "            validated_tags.append(clean_tag)\n",
    "\n",
    "    validated_tags = list(dict.fromkeys(validated_tags))\n",
    "    \n",
    "    query = \" \".join(validated_tags)\n",
    "    if country:\n",
    "        country_name = COUNTRY_SEARCH_NAMES.get(country, country)\n",
    "        if country_name.lower() not in query.lower():\n",
    "            query = f\"{query} {country_name}\"\n",
    "    \n",
    "    return query\n",
    "\n",
    "def perform_web_search(query, max_results=3):\n",
    "    \"\"\"Search the web using DuckDuckGo.\"\"\"\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = list(ddgs.text(\n",
    "                query, \n",
    "                max_results=max_results,\n",
    "                backend=\"api\"\n",
    "            ))\n",
    "        if not results: \n",
    "            with DDGS() as ddgs:\n",
    "                results = list(ddgs.text(query, max_results=max_results, backend=\"html\"))\n",
    "        if not results: \n",
    "            return \"\"\n",
    "        return \"\\n\\n\".join([f\"[Web Source: {r['title']}]\\n{r['body']}\" for r in results])\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def check_relevance(question, context, model, tokenizer):\n",
    "    \"\"\"Ask model if context contains the answer.\"\"\"\n",
    "    prompt = f\"[INST] Context:\\n{context}\\n\\nQuestion: {question}\\n\\nDoes the provided context contain the answer to the question? Answer ONLY with 'YES' or 'NO'. [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=4, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "    resp = tokenizer.decode(out[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True).strip().upper()\n",
    "    return \"YES\" in resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAQ generation with RAG fallback\n",
    "\n",
    "def build_saq_user_message(question: str, context: str = None) -> str:\n",
    "    \"\"\"Build SAQ user message with optional Wikipedia context placed AFTER few-shot.\"\"\"\n",
    "    context_block = \"\"\n",
    "    instruction_add = \"\"\n",
    "    if context:\n",
    "        context_block = f\"Reference Information:\\n{context}\\n\\n\"\n",
    "        instruction_add = \"Based strictly on the Reference Information above, \"\n",
    "    \n",
    "    return (\n",
    "        \"Read the following question and provide a single answer \"\n",
    "        \"without any explanations.\\n\\n\"\n",
    "        f\"{SAQ_FEW_SHOT}\"\n",
    "        f\"{context_block}\"\n",
    "        f\"{instruction_add}\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_answer_with_confidence(prompt: str, model, tokenizer, max_tokens: int = 10) -> Tuple[str, float]:\n",
    "    \"\"\"Generate answer and calculate confidence score from token probabilities.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_ids = outputs.sequences[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    confidences = []\n",
    "    for i, score in enumerate(outputs.scores):\n",
    "        if i >= len(generated_ids):\n",
    "            break\n",
    "        probs = torch.softmax(score[0], dim=-1)\n",
    "        token_prob = probs[generated_ids[i]].item()\n",
    "        confidences.append(token_prob)\n",
    "    \n",
    "    avg_confidence = np.mean(confidences) if confidences else 0.0\n",
    "    answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "    return answer, avg_confidence\n",
    "\n",
    "\n",
    "def saq_generate_with_rag(\n",
    "    question: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    rag,\n",
    "    country: str = None,\n",
    "    confidence_threshold: float = 0.7,\n",
    "    top_k: int = 3,\n",
    "    max_context_chars: int = 3500\n",
    ") -> Tuple[str, bool, Optional[str]]:\n",
    "    \"\"\"Generate SAQ answer using confidence-based RAG fallback + Web Search.\"\"\"\n",
    "    \n",
    "    base_user_message = build_saq_user_message(question)\n",
    "    base_prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": base_user_message}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    direct_answer, confidence = get_answer_with_confidence(\n",
    "        base_prompt, model, tokenizer, max_tokens=10\n",
    "    )\n",
    "    \n",
    "    if confidence >= confidence_threshold:\n",
    "        return direct_answer, False, None\n",
    "    \n",
    "    retrieved = rag.retrieve(question, top_k=top_k)\n",
    "    \n",
    "    context_parts = []\n",
    "    for i, (passage, _) in enumerate(retrieved):\n",
    "        context_parts.append(f\"[Document {i+1}]\\n{passage}\")\n",
    "        \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    if len(context) > max_context_chars:\n",
    "        context = context[:max_context_chars] + \"... (truncated)\"\n",
    "        \n",
    "    is_relevant = check_relevance(question, context, model, tokenizer)\n",
    "    \n",
    "    if not is_relevant:\n",
    "        search_query = generate_search_query(question, model, tokenizer, country=country)\n",
    "        web_context = perform_web_search(search_query, max_results=3)\n",
    "        if web_context:\n",
    "            context = web_context\n",
    "            \n",
    "    rag_user_message = build_saq_user_message(question, context=context)\n",
    "    rag_prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": rag_user_message}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(rag_prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=15,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    rag_answer = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "\n",
    "    return rag_answer if rag_answer else direct_answer, True, context\n",
    "\n",
    "\n",
    "def normalize_answer(answer: str) -> str:\n",
    "    answer = answer.lower().strip()\n",
    "    answer = answer.rstrip(\".,;:!?\")\n",
    "    answer = re.sub(r'^(the|a|an)\\s+', '', answer)\n",
    "    return ' '.join(answer.split())\n",
    "\n",
    "\n",
    "test_q = \"What is the capital of Iran?\"\n",
    "answer, used_rag, context = saq_generate_with_rag(test_q, model, tokenizer, wiki_rag, country=\"IR\", confidence_threshold=0.7)\n",
    "print(f\"Test: {test_q} â†’ {normalize_answer(answer)} (RAG used: {used_rag})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b089fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SAQ predictions on test set\n",
    "\n",
    "saq_test = pd.read_csv(\"test_dataset_saq.csv\")[[\"ID\", \"en_question\", \"country\"]]\n",
    "saq_test[\"country_code\"] = saq_test[\"country\"].map(COUNTRY_CODE_MAP).fillna(saq_test[\"country\"])\n",
    "\n",
    "saq_preds = []\n",
    "rag_usage_count = 0\n",
    "model.eval()\n",
    "\n",
    "for i, row in saq_test.iterrows():\n",
    "    answer, used_rag, context = saq_generate_with_rag(\n",
    "        question=row[\"en_question\"],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        rag=wiki_rag,\n",
    "        country=row[\"country_code\"],\n",
    "        confidence_threshold=0.7,\n",
    "        top_k=3,\n",
    "        max_context_chars=3500\n",
    "    )\n",
    "    answer = normalize_answer(answer)\n",
    "    saq_preds.append(answer)\n",
    "    \n",
    "    if used_rag:\n",
    "        rag_usage_count += 1\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(f\"\\rProgress: {i+1}/{len(saq_test)} | RAG used: {rag_usage_count}\", end=\"\", flush=True)\n",
    "\n",
    "saq_test[\"answer\"] = saq_preds\n",
    "\n",
    "saq_submission = saq_test[[\"ID\", \"answer\"]]\n",
    "saq_submission.loc[saq_submission[\"answer\"] == \"\", \"answer\"] = \"none\"\n",
    "saq_submission.to_csv(\"saq_prediction.tsv\", sep='\\t', index=False)\n",
    "\n",
    "print(f\"\\n\\nSAQ predictions complete!\")\n",
    "print(f\"Total: {len(saq_test)} | RAG used: {rag_usage_count} ({100*rag_usage_count/len(saq_test):.1f}%)\")\n",
    "print(f\"Saved to saq_prediction.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a56d2e",
   "metadata": {},
   "source": [
    "# MCQ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0624d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MCQ training data\n",
    "\n",
    "mcq_train_df = pd.read_csv(\"train_dataset_mcq.csv\")\n",
    "mcq_train_df[\"choices\"] = mcq_train_df[\"choices\"].apply(json.loads)\n",
    "\n",
    "def safe_json_parse(x):\n",
    "    try:\n",
    "        return json.loads(x) if isinstance(x, str) else x\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "mcq_train_df[\"choice_countries\"] = mcq_train_df[\"choice_countries\"].apply(safe_json_parse)\n",
    "mcq_train_df[\"country_code\"] = mcq_train_df[\"country\"].map(COUNTRY_CODE_MAP).fillna(mcq_train_df[\"country\"])\n",
    "\n",
    "print(f\"MCQ training samples: {len(mcq_train_df)}\")\n",
    "print(\"\\nCountry distribution:\")\n",
    "print(mcq_train_df['country_code'].value_counts())\n",
    "mcq_train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee6936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCQ dataset with country-specific few-shot examples\n",
    "\n",
    "def format_mcq_prompt(row) -> str:\n",
    "    \"\"\"Format a MCQ row into a standardized prompt string.\"\"\"\n",
    "    choices = row[\"choices\"]\n",
    "    return (\n",
    "        \"Answer the following multiple choice question.\\n\"\n",
    "        \"Choose exactly one option (A, B, C, or D).\\n\\n\"\n",
    "        f\"{row['prompt']}\\n\\n\"\n",
    "        f\"A. {choices['A']}\\n\"\n",
    "        f\"B. {choices['B']}\\n\"\n",
    "        f\"C. {choices['C']}\\n\"\n",
    "        f\"D. {choices['D']}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "MCQ_TRAIN_FEW_SHOT = {\n",
    "    \"IR\": \"\"\"Example:\n",
    "What is the traditional New Year celebration in Iran?\n",
    "A. Eid\n",
    "B. Diwali\n",
    "C. Nowruz\n",
    "D. Hanukkah\n",
    "Answer: C\n",
    "\n",
    "\"\"\",\n",
    "    \"CN\": \"\"\"Example:\n",
    "What is the capital of China?\n",
    "A. Shanghai\n",
    "B. Beijing\n",
    "C. Hong Kong\n",
    "D. Guangzhou\n",
    "Answer: B\n",
    "\n",
    "\"\"\",\n",
    "    \"GB\": \"\"\"Example:\n",
    "What is the currency of the United Kingdom?\n",
    "A. Euro\n",
    "B. Dollar\n",
    "C. Pound Sterling\n",
    "D. Franc\n",
    "Answer: C\n",
    "\n",
    "\"\"\",\n",
    "    \"US\": \"\"\"Example:\n",
    "What is the capital of the United States?\n",
    "A. New York\n",
    "B. Los Angeles\n",
    "C. Washington D.C.\n",
    "D. Chicago\n",
    "Answer: C\n",
    "\n",
    "\"\"\",\n",
    "\n",
    "    \"default\":\"\"\n",
    "}\n",
    "\n",
    "COUNTRY_NAMES_MCQ = {\"IR\": \"Iran\", \"GB\": \"UK\", \"CN\": \"China\", \"US\": \"US\"}\n",
    "\n",
    "def preprocess_mcq_batch(batch, tokenizer, max_length=768):\n",
    "    \"\"\"Preprocess a batch of MCQ examples for training.\"\"\"\n",
    "    input_ids_list = []\n",
    "    attention_masks_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for i in range(len(batch[\"prompt\"])):\n",
    "        row_prompt = batch[\"prompt\"][i]\n",
    "        choices = batch[\"choices\"][i]\n",
    "        correct_answer = batch[\"answer_idx\"][i].strip()\n",
    "        country = batch[\"country_code\"][i] if \"country_code\" in batch else batch[\"country\"][i]\n",
    "        \n",
    "        few_shot = MCQ_TRAIN_FEW_SHOT.get(country, \"\")\n",
    "        \n",
    "        country_context = \"\"\n",
    "        if country:\n",
    "            country_name = COUNTRY_NAMES_MCQ.get(country, country)\n",
    "            country_context = f\"Context: This question is about {country_name}.\\n\\n\"\n",
    "\n",
    "        formatted_prompt = (\n",
    "            \"Answer the following multiple choice question.\\n\"\n",
    "            \"Choose exactly one option (A, B, C, or D).\\n\\n\"\n",
    "            f\"{few_shot}\"\n",
    "            f\"{country_context}\"\n",
    "            f\"{row_prompt}\\n\\n\"\n",
    "            f\"A. {choices['A']}\\n\"\n",
    "            f\"B. {choices['B']}\\n\"\n",
    "            f\"C. {choices['C']}\\n\"\n",
    "            f\"D. {choices['D']}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "\n",
    "        full_prompt_str = f\"<s>[INST] {formatted_prompt} [/INST]\"\n",
    "        full_text = full_prompt_str + \" \" + correct_answer + tokenizer.eos_token\n",
    "\n",
    "        tokenized_full = tokenizer(\n",
    "            full_text,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized_full[\"input_ids\"][0]\n",
    "        attention_mask = tokenized_full[\"attention_mask\"][0]\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        tokenized_prompt = tokenizer(\n",
    "            full_prompt_str,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        prompt_len = min(tokenized_prompt[\"input_ids\"].shape[1], max_length)\n",
    "\n",
    "        labels[:prompt_len] = -100\n",
    "        labels[attention_mask == 0] = -100\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_masks_list.append(attention_mask)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_masks_list,\n",
    "        \"labels\": labels_list\n",
    "    }\n",
    "\n",
    "mcq_hf_dataset = HFDataset.from_pandas(mcq_train_df)\n",
    "mcq_tokenized_dataset = mcq_hf_dataset.map(\n",
    "    lambda batch: preprocess_mcq_batch(batch, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=mcq_hf_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Tokenized {len(mcq_tokenized_dataset)} MCQ samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127cffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq_lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "if hasattr(model, 'unload'):\n",
    "    model.unload()\n",
    "    \n",
    "model = get_peft_model(model, mcq_lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54242fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MCQ model\n",
    "mcq_training_args = TrainingArguments(\n",
    "    output_dir=\"./mcq-lora-adapter-clean\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "mcq_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=mcq_training_args,\n",
    "    train_dataset=mcq_tokenized_dataset\n",
    ")\n",
    "\n",
    "mcq_trainer.train()\n",
    "print(\"MCQ training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98fc0b4",
   "metadata": {},
   "source": [
    "# MCQ Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b48d136-c9a2-4fe6-8f3d-e00e00843b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_base_model(\"./mcq-lora-adapter-clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCQ generation with RAG fallback\n",
    "\n",
    "def extract_mcq_choice(answer: str) -> str:\n",
    "    \"\"\"Extract the choice letter (A, B, C, or D) from model output.\"\"\"\n",
    "    json_match = re.search(r'\"answer_choice\"\\s*:\\s*\"([A-D])\"', answer)\n",
    "    if json_match:\n",
    "        return json_match.group(1)\n",
    "    \n",
    "    matches = re.findall(r'\\b[A-D]\\b', answer.upper())\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    \n",
    "    for char in answer.upper():\n",
    "        if char in 'ABCD':\n",
    "            return char\n",
    "    \n",
    "    return \"A\"\n",
    "\n",
    "\n",
    "def build_mcq_prompt(question: str, choices: dict, country: str = None, context: str = None) -> str:\n",
    "    \"\"\"Build MCQ prompt with optional Wikipedia context and country-specific few-shot.\"\"\"\n",
    "    country_context = \"\"\n",
    "    if country:\n",
    "        country_name = COUNTRY_NAMES_MCQ.get(country, country)\n",
    "        country_context = f\"Context: This question is about {country_name}.\\n\\n\"\n",
    "    \n",
    "    few_shot = MCQ_TRAIN_FEW_SHOT.get(country, MCQ_TRAIN_FEW_SHOT[\"default\"])\n",
    "    \n",
    "    context_block = \"\"\n",
    "    instruction_add = \"\"\n",
    "    if context:\n",
    "        context_block = f\"Reference Information:\\n{context}\\n\\n\"\n",
    "        instruction_add = \"Based strictly on the Reference Information above, \"\n",
    "    \n",
    "    formatted_prompt = (\n",
    "        \"Answer the following multiple choice question.\\n\"\n",
    "        \"Choose exactly one option (A, B, C, or D).\\n\\n\"\n",
    "        f\"{few_shot}\"\n",
    "        f\"{country_context}\"\n",
    "        f\"{context_block}\"\n",
    "        f\"{instruction_add}\"\n",
    "        f\"{question}\"\n",
    "\n",
    "    )\n",
    "\n",
    "    return f\"<s>[INST] {formatted_prompt} [/INST]\"\n",
    "\n",
    "\n",
    "def mcq_generate_with_rag(\n",
    "    question: str,\n",
    "    choices: dict,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    rag,\n",
    "    country: str = None,\n",
    "    confidence_threshold: float = 0.8,\n",
    "    top_k: int = 3,\n",
    "    max_context_chars: int = 3500\n",
    ") -> Tuple[str, bool, Optional[str]]:\n",
    "    \"\"\"Generate MCQ answer using confidence-based RAG fallback + Web Search.\"\"\"\n",
    "    \n",
    "    base_prompt = build_mcq_prompt(question, choices, country=country)\n",
    "    direct_answer, confidence = get_answer_with_confidence(\n",
    "        base_prompt, model, tokenizer, max_tokens=2\n",
    "    )\n",
    "    if confidence >= confidence_threshold:\n",
    "        return direct_answer, False, None\n",
    "    \n",
    "    retrieved = rag.retrieve(question, top_k=top_k, country=country)\n",
    "    \n",
    "    context_parts = []\n",
    "    for i, (passage, _) in enumerate(retrieved):\n",
    "        context_parts.append(f\"[Document {i+1}]\\n{passage}\")\n",
    "        \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    if len(context) > max_context_chars:\n",
    "        context = context[:max_context_chars] + \"... (truncated)\"\n",
    "\n",
    "    is_relevant = check_relevance(question, context, model, tokenizer)\n",
    "    \n",
    "    if not is_relevant:\n",
    "        search_query = generate_search_query(question, model, tokenizer, country=country)\n",
    "        web_context = perform_web_search(search_query, max_results=3)\n",
    "        if web_context:\n",
    "            context = web_context\n",
    "  \n",
    "    final_prompt_content = build_mcq_prompt(question, choices, country=country, context=context)\n",
    "    \n",
    "    inputs = tokenizer(final_prompt_content, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    rag_answer = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "        \n",
    "    return rag_answer if rag_answer else direct_answer, True, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8975645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MCQ predictions on test set\n",
    "\n",
    "mcq_test = pd.read_csv(\"test_dataset_mcq.csv\")\n",
    "mcq_test[\"choices\"] = mcq_test[\"choices\"].apply(safe_json_parse)\n",
    "mcq_test[\"country_code\"] = mcq_test[\"country\"].map(COUNTRY_CODE_MAP).fillna(mcq_test[\"country\"])\n",
    "\n",
    "mcq_preds = []\n",
    "mcq_rag_count = 0\n",
    "model.eval()\n",
    "\n",
    "for i, row in mcq_test.iterrows():\n",
    "    answer, used_rag, context = mcq_generate_with_rag(\n",
    "        question=row[\"prompt\"],\n",
    "        choices=row[\"choices\"],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        rag=wiki_rag,\n",
    "        country=row[\"country_code\"],\n",
    "        confidence_threshold=0.8,\n",
    "        top_k=3, \n",
    "        max_context_chars=3500\n",
    "    )\n",
    "    choice = extract_mcq_choice(answer)\n",
    "    mcq_preds.append(choice)\n",
    "    if used_rag:\n",
    "        mcq_rag_count += 1\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(f\"\\rProgress: {i+1}/{len(mcq_test)} | RAG used: {mcq_rag_count}\", end=\"\", flush=True)\n",
    "\n",
    "mcq_test[\"choice\"] = mcq_preds\n",
    "   \n",
    "mcq_submission = pd.get_dummies(mcq_test[\"choice\"]).astype(bool)\n",
    "for col in ['A', 'B', 'C', 'D']:\n",
    "    if col not in mcq_submission.columns:\n",
    "        mcq_submission[col] = False\n",
    "\n",
    "mcq_submission = pd.concat([\n",
    "    mcq_test[\"MCQID\"], \n",
    "    mcq_submission[['A', 'B', 'C', 'D']]\n",
    "], axis=1)\n",
    "\n",
    "mcq_submission.to_csv(\"mcq_prediction.tsv\", sep='\\t', index=False)\n",
    "\n",
    "print(f\"MCQ predictions complete!\")\n",
    "print(f\"Total: {len(mcq_test)} | RAG used: {mcq_rag_count} ({100*mcq_rag_count/len(mcq_test):.1f}%)\")\n",
    "print(f\"Saved to mcq_prediction.tsv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
